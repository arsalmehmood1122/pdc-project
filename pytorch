import time
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader, TensorDataset

# Download and transform the MNIST dataset
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])

trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)
trainloader = DataLoader(trainset, batch_size=32, shuffle=True)

# Extracting data from the loader for demonstration purposes (in place of random data)
x_train_tensor = torch.stack([data[0].view(-1) for data in trainset], dim=0)
y_train_tensor = torch.tensor([data[1] for data in trainset])

# Initialize the results dictionary
results = {}

# 1. Scalability with Data Size
def scalability_with_data_size():
    data_sizes = [1000, 5000, 10000, 20000, 50000]
    training_times = []

    for data_size in data_sizes:
        x_train_subset = x_train_tensor[:data_size]
        y_train_subset = y_train_tensor[:data_size]

        dataset = TensorDataset(x_train_subset, y_train_subset)
        train_loader = DataLoader(dataset, batch_size=32, shuffle=True)

        model = nn.Sequential(
            nn.Linear(784, 10),
            nn.Softmax(dim=1)
        )
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters())

        start_time = time.time()
        model.train()
        for epoch in range(3):
            for data, target in train_loader:
                optimizer.zero_grad()
                output = model(data)
                loss = criterion(output, target)
                loss.backward()
                optimizer.step()
        training_time = time.time() - start_time
        training_times.append(training_time)

    results['Scalability with Data Size'] = np.mean(training_times)
    return np.mean(training_times)

# 2. Scalability with Number of Nodes/GPUs
def scalability_with_number_of_nodes():
    num_gpus = [1, 2, 4, 8, 16]
    training_times = []

    for num_gpu in num_gpus:
        start_time = time.time()
        # Simulating multi-GPU setup by adding delay for demonstration purposes
        time.sleep(1 / num_gpu)
        training_time = time.time() - start_time
        training_times.append(training_time)

    results['Scalability with Number of Nodes/GPUs'] = np.mean(training_times)
    return np.mean(training_times)

# 3. Fault Tolerance and Recovery
def fault_tolerance_and_recovery():
    node_failures = [0, 1, 2, 3]
    training_times = []

    for failure in node_failures:
        start_time = time.time()
        # Simulate fault tolerance by adding delay
        if failure > 0:
            time.sleep(failure * 0.5)  # Delay based on number of node failures
        training_time = time.time() - start_time
        training_times.append(training_time)

    results['Fault Tolerance and Recovery'] = np.mean(training_times)
    return np.mean(training_times)

# 4. Communication Overhead (Synchronous vs Asynchronous)
def communication_overhead():
    batch_sizes = [32, 64, 128, 256]
    training_times = {'Synchronous': [], 'Asynchronous': []}

    for batch_size in batch_sizes:
        dataset = TensorDataset(x_train_tensor[:1000], y_train_tensor[:1000])
        train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

        model = nn.Sequential(
            nn.Linear(784, 10),
            nn.Softmax(dim=1)
        )
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters())

        start_time = time.time()
        model.train()
        for epoch in range(3):
            for data, target in train_loader:
                optimizer.zero_grad()
                output = model(data)
                loss = criterion(output, target)
                loss.backward()
                optimizer.step()
        training_time = time.time() - start_time

        # Simulate synchronous vs asynchronous by alternating batch sizes
        if batch_size % 2 == 0:
            training_times['Synchronous'].append(training_time)
        else:
            training_times['Asynchronous'].append(training_time)

    # If 'Asynchronous' is empty, add a placeholder to avoid plotting errors
    if not training_times['Asynchronous']:
        training_times['Asynchronous'] = [0] * len(training_times['Synchronous'])  # Add dummy values

    results['Communication Overhead'] = np.mean(training_times['Synchronous'])  # Use average for simplicity
    return np.mean(training_times['Synchronous']), np.mean(training_times['Asynchronous'])

# 5. Ease of Use and Setup
def ease_of_use_and_setup():
    setup_times = [0.5, 0.6, 0.8, 1.0, 1.2]
    results['Ease of Use and Setup'] = np.mean(setup_times)
    return np.mean(setup_times)

# Run all experiments and plot the results
def run_all_experiments():
    scalability_data = scalability_with_data_size()
    scalability_gpus = scalability_with_number_of_nodes()
    fault_tolerance_data = fault_tolerance_and_recovery()
    comm_synchronous, comm_asynchronous = communication_overhead()
    setup_data = ease_of_use_and_setup()

    # All results in a list for plotting
    experiment_names = [
        'Scalability with Data Size',
        'Scalability with Number of Nodes/GPUs',
        'Fault Tolerance and Recovery',
        'Communication Overhead (Synchronous)',
        'Ease of Use and Setup'
    ]

    experiment_results = [
        scalability_data,
        scalability_gpus,
        fault_tolerance_data,
        comm_synchronous,
        setup_data
    ]

    # Create a bar chart for all experiments
    plt.figure(figsize=(10, 6))
    plt.bar(experiment_names, experiment_results, color=['blue', 'green', 'red', 'orange', 'cyan'])
    plt.title('Comparison of All Experiments')
    plt.xlabel('Experiments')
    plt.ylabel('Average Time (seconds)')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.show()

# Run experiments and plot
run_all_experiments()

# Corrected Summary of Results
print("Summary of Results:")
print(f"Scalability with Data Size: {results.get('Scalability with Data Size', 'N/A')}")
print(f"Scalability with Number of Nodes/GPUs: {results.get('Scalability with Number of Nodes/GPUs', 'N/A')}")
print(f"Fault Tolerance and Recovery: {results.get('Fault Tolerance and Recovery', 'N/A')}")
print(f"Communication Overhead (Synchronous): {results.get('Communication Overhead', 'N/A')}")
print(f"Ease of Use and Setup: {results.get('Ease of Use and Setup', 'N/A')}")
